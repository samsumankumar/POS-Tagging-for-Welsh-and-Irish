{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\samsu\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=8, verbose=True, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}\\n')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ... \\n')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To test a single line data for visulaisation \n",
    "def compute_accuracy_test(output, gold):\n",
    "    try:\n",
    "        assert(len(output) == len(gold))\n",
    "    except:\n",
    "        print(\"Different number of words in the two lists!\")\n",
    "        return -1\n",
    "\n",
    "    count_correct = 0\n",
    "    count_total_tokens = 0\n",
    "    for o_sent,g_sent in zip(output,gold):\n",
    "        check = [o_token == g_token for o_token,g_token in zip(o_sent,g_sent)]\n",
    "        count_correct += sum(check)\n",
    "        count_total_tokens += len(check)\n",
    "    return count_correct/count_total_tokens\n",
    "\n",
    "def compute_accuracy(output, gold):\n",
    "    try:\n",
    "        assert(len(output) == len(gold))\n",
    "    except:\n",
    "        print(\"Different number of lines in the two files!\")\n",
    "        return -1\n",
    "\n",
    "    count_correct = 0\n",
    "    count_total_tokens = 0\n",
    "    for o_sent,g_sent in zip(output,gold):\n",
    "        try:\n",
    "            assert(len(o_sent)==len(g_sent))\n",
    "        except:\n",
    "            print(\"Different number of tokens in the two lines!\")\n",
    "            return -1\n",
    "        check = [o_token[1] == g_token[1] for o_token,g_token in zip(o_sent,g_sent)]\n",
    "        count_correct += sum(check)\n",
    "        count_total_tokens += len(check)\n",
    "    return count_correct/count_total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and preparing the file for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Reading the file====================\n",
      "====================Reading the file====================\n",
      "====================Reading the file====================\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#encoding=\"ISO-8859-1\"\n",
    "def read_data(f):\n",
    "    with open(f, encoding = \"utf-8\",) as inp:\n",
    "        lines = inp.readlines()\n",
    "        if len(lines) == 0:\n",
    "            raise ValueError('This file is empty')\n",
    "        else:\n",
    "            print('='*20+'Reading the file'+'='*20)\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        line = line.strip().split()\n",
    "        sentence = []\n",
    "        for token in line:\n",
    "            token = token.split('|')\n",
    "            word = token[0]\n",
    "            tag = token[1]\n",
    "            sentence.append((word,tag))\n",
    "        data.append(sentence)\n",
    "    return data\n",
    "\n",
    "def convert_data_for_training(data):\n",
    "    for d in data:\n",
    "        tokens = [t[0] for t in d]\n",
    "        tags = [t[1] for t in d]\n",
    "    return [([t[0] for t in d],[t[1] for t in d]) for d in data]\n",
    "\n",
    "TRAINING_FILE = \"data/irish.train\"\n",
    "training_data = convert_data_for_training(read_data(TRAINING_FILE))\n",
    "\n",
    "DEV_FILE = \"data/irish.dev\"\n",
    "dev_data = convert_data_for_training(read_data(DEV_FILE))\n",
    "\n",
    "TEST_FILE = \"data/irish.test\"\n",
    "test_data = convert_data_for_training(read_data(TEST_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare words are assigned as UNK \n",
    "Run the Below cell if you want to replace the rare words with 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_with_UNK(data, n=1):\n",
    "    \n",
    "    doc = ''\n",
    "    for i in range(len(data)):\n",
    "        s = ' '.join(data[i][0])\n",
    "        doc = doc + ' ' + s\n",
    "    \n",
    "    words = doc.lower().split()    \n",
    "    wordfreq = [words.count(p) for p in words]\n",
    "    words_UNK = words.copy()\n",
    "    #freq_dict = dict(list(zip(words,wordfreq)))\n",
    "    \n",
    "    # Find all words that appear <= n times in the corpus\n",
    "    # Iterate through the corpus and substitute the rare words with UNK\n",
    "    for i in range(len(wordfreq)):\n",
    "        if(wordfreq[i]==n):\n",
    "            words_UNK[i] = 'UNK'\n",
    "    data = [words,words_UNK]\n",
    "    return data\n",
    "\n",
    "data = substitute_with_UNK(training_data, n=1)\n",
    "words = data[0].copy()\n",
    "words_unk = data[1].copy()\n",
    "\n",
    "word_to_ix = {}\n",
    "ix_to_word = {}\n",
    "for word in words_unk:\n",
    "    if word not in word_to_ix:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "        ix_to_word[word_to_ix[word]] = word\n",
    "\n",
    "tag_to_ix = {}\n",
    "ix_to_tag = {}\n",
    "for sent, tags in training_data:\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "            ix_to_tag[tag_to_ix[tag]] = tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the words and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    sequence = []\n",
    "    for w in seq:\n",
    "        if w in to_ix:\n",
    "            sequence.append(to_ix[w])\n",
    "        else:\n",
    "            sequence.append(to_ix['UNK'])\n",
    "    return sequence\n",
    "\n",
    "def reverse_sequence(seq, to_word):\n",
    "    sentence = []\n",
    "    for w in seq:\n",
    "        if w in to_word:\n",
    "            sentence.append(to_word[w])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, dev and test data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First convert the sentences to sequences, then put them in a list.\n",
    "Then check if they are in tensor type, use 'TensorDataset(inps, tgts)' to bind x \n",
    "and y where all the lists should be of same length.\n",
    "Then use dataloader for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    for sentence, tag in data:\n",
    "        sentences.append(torch.LongTensor(prepare_sequence(sentence, word_to_ix)))\n",
    "        tags.append(torch.LongTensor(prepare_sequence(tag, tag_to_ix)))\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "def collate(batch):\n",
    "    sentences, tags = preprocess(batch)\n",
    "    X = pad_sequence(sentences, batch_first= True)\n",
    "    y = pad_sequence(tags, batch_first= True)\n",
    "    return X, y\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size = batch_size, collate_fn = collate)\n",
    "dev_loader = DataLoader(dev_data, batch_size = batch_size, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_rev(data):\n",
    "    rev_idx = torch.arange(data.size(1)-1, -1, -1).long()\n",
    "    rev = data.index_select(1, rev_idx)\n",
    "    return rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsu\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\samsu\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "ft_irish = FastText.load_fasttext_format(r'C:\\Users\\samsu\\Desktop\\Fall20\\CS 695-002\\Assignment 1\\part2\\part2d\\cc.cy.300.bin')\n",
    "ft_welsh = FastText.load_fasttext_format(r'C:\\Users\\samsu\\Desktop\\Fall20\\CS 695-002\\Assignment 1\\part2\\part2d\\cc.ga.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torch.FloatTensor(ft_irish.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim1, hidden_dim2, vocab_size, output_size, vectors):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(vectors)\n",
    "        \n",
    "        self.lstm_forward1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first = True)\n",
    "        self.lstm_backward1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first = True)\n",
    "        \n",
    "        self.lstm_forward2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first = True)\n",
    "        self.lstm_backward2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first = True)\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim2, output_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        embeds = self.word_embeddings(sentence)\n",
    "              \n",
    "        lstm_out0, _ = self.lstm_forward1(embeds)\n",
    "        data_rev = input_rev(embeds)\n",
    "        lstm_out1, _ = self.lstm_backward1(data_rev)\n",
    "        lstm_out_rev = input_rev(lstm_out1)\n",
    "        lstm_out = lstm_out0 + lstm_out_rev\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        lstm_out0, _ = self.lstm_forward2(lstm_out)\n",
    "        data_rev = input_rev(lstm_out)\n",
    "        lstm_out1, _ = self.lstm_backward2(data_rev)\n",
    "        lstm_out_rev = input_rev(lstm_out1)\n",
    "        lstm_out = lstm_out0 + lstm_out_rev\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "              \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim2)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_space = self.dropout(tag_space)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "        return tag_scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, patience, n_epochs):\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Starting epoch {epoch+1}...\")\n",
    "        #Sets the model in training mode where training is set to True to utilise any\n",
    "        #regularisation/BatchNormalization if available.  \n",
    "        model.train()\n",
    "        for sentence, tags in train_loader:\n",
    "            model.zero_grad()\n",
    "        \n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, tags.flatten())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        #Sets the model in evaluation mode where training is set to False to avoid any \n",
    "        #regularisation/BatchNormalization if available.    \n",
    "        model.eval() \n",
    "        for sentence, tags in dev_loader:\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, tags.flatten())\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print(f'[{epoch+1:>{epoch_len}}/{n_epochs:>{epoch_len}}]' +'-'*20+'->'+\n",
    "                     f'train_loss: {train_loss:.5f}   ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        # Empty lists again for the next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        early_stopping(valid_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\"+'-'*10 +\"STOPPED\")\n",
    "            \n",
    "            break\n",
    "        \n",
    "    # loading the last best the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1...\n",
      "[ 1/50]--------------------->train_loss: 1.43971   valid_loss: 0.64238\n",
      "Validation loss decreased (inf --> 0.642382).  Saving model ... \n",
      "\n",
      "Starting epoch 2...\n",
      "[ 2/50]--------------------->train_loss: 1.14963   valid_loss: 0.41324\n",
      "Validation loss decreased (0.642382 --> 0.413236).  Saving model ... \n",
      "\n",
      "Starting epoch 3...\n",
      "[ 3/50]--------------------->train_loss: 1.06179   valid_loss: 0.36664\n",
      "Validation loss decreased (0.413236 --> 0.366638).  Saving model ... \n",
      "\n",
      "Starting epoch 4...\n",
      "[ 4/50]--------------------->train_loss: 1.02378   valid_loss: 0.31411\n",
      "Validation loss decreased (0.366638 --> 0.314106).  Saving model ... \n",
      "\n",
      "Starting epoch 5...\n",
      "[ 5/50]--------------------->train_loss: 1.00573   valid_loss: 0.30694\n",
      "Validation loss decreased (0.314106 --> 0.306942).  Saving model ... \n",
      "\n",
      "Starting epoch 6...\n",
      "[ 6/50]--------------------->train_loss: 0.99001   valid_loss: 0.30022\n",
      "Validation loss decreased (0.306942 --> 0.300221).  Saving model ... \n",
      "\n",
      "Starting epoch 7...\n",
      "[ 7/50]--------------------->train_loss: 0.97939   valid_loss: 0.27817\n",
      "Validation loss decreased (0.300221 --> 0.278166).  Saving model ... \n",
      "\n",
      "Starting epoch 8...\n",
      "[ 8/50]--------------------->train_loss: 0.96557   valid_loss: 0.27375\n",
      "Validation loss decreased (0.278166 --> 0.273751).  Saving model ... \n",
      "\n",
      "Starting epoch 9...\n",
      "[ 9/50]--------------------->train_loss: 0.96182   valid_loss: 0.26899\n",
      "Validation loss decreased (0.273751 --> 0.268988).  Saving model ... \n",
      "\n",
      "Starting epoch 10...\n",
      "[10/50]--------------------->train_loss: 0.94866   valid_loss: 0.26524\n",
      "Validation loss decreased (0.268988 --> 0.265245).  Saving model ... \n",
      "\n",
      "Starting epoch 11...\n",
      "[11/50]--------------------->train_loss: 0.94493   valid_loss: 0.26247\n",
      "Validation loss decreased (0.265245 --> 0.262473).  Saving model ... \n",
      "\n",
      "Starting epoch 12...\n",
      "[12/50]--------------------->train_loss: 0.94883   valid_loss: 0.25142\n",
      "Validation loss decreased (0.262473 --> 0.251417).  Saving model ... \n",
      "\n",
      "Starting epoch 13...\n",
      "[13/50]--------------------->train_loss: 0.93276   valid_loss: 0.24230\n",
      "Validation loss decreased (0.251417 --> 0.242297).  Saving model ... \n",
      "\n",
      "Starting epoch 14...\n",
      "[14/50]--------------------->train_loss: 0.92394   valid_loss: 0.24843\n",
      "EarlyStopping counter: 1 out of 7\n",
      "\n",
      "Starting epoch 15...\n",
      "[15/50]--------------------->train_loss: 0.91772   valid_loss: 0.23829\n",
      "Validation loss decreased (0.242297 --> 0.238288).  Saving model ... \n",
      "\n",
      "Starting epoch 16...\n",
      "[16/50]--------------------->train_loss: 0.91045   valid_loss: 0.24772\n",
      "EarlyStopping counter: 1 out of 7\n",
      "\n",
      "Starting epoch 17...\n",
      "[17/50]--------------------->train_loss: 0.90544   valid_loss: 0.24404\n",
      "EarlyStopping counter: 2 out of 7\n",
      "\n",
      "Starting epoch 18...\n",
      "[18/50]--------------------->train_loss: 0.90187   valid_loss: 0.23478\n",
      "Validation loss decreased (0.238288 --> 0.234779).  Saving model ... \n",
      "\n",
      "Starting epoch 19...\n",
      "[19/50]--------------------->train_loss: 0.90336   valid_loss: 0.23456\n",
      "Validation loss decreased (0.234779 --> 0.234561).  Saving model ... \n",
      "\n",
      "Starting epoch 20...\n",
      "[20/50]--------------------->train_loss: 0.89036   valid_loss: 0.24253\n",
      "EarlyStopping counter: 1 out of 7\n",
      "\n",
      "Starting epoch 21...\n",
      "[21/50]--------------------->train_loss: 0.89510   valid_loss: 0.24652\n",
      "EarlyStopping counter: 2 out of 7\n",
      "\n",
      "Starting epoch 22...\n",
      "[22/50]--------------------->train_loss: 0.89773   valid_loss: 0.23384\n",
      "Validation loss decreased (0.234561 --> 0.233844).  Saving model ... \n",
      "\n",
      "Starting epoch 23...\n",
      "[23/50]--------------------->train_loss: 0.88658   valid_loss: 0.23601\n",
      "EarlyStopping counter: 1 out of 7\n",
      "\n",
      "Starting epoch 24...\n",
      "[24/50]--------------------->train_loss: 0.89085   valid_loss: 0.24710\n",
      "EarlyStopping counter: 2 out of 7\n",
      "\n",
      "Starting epoch 25...\n",
      "[25/50]--------------------->train_loss: 0.89493   valid_loss: 0.24103\n",
      "EarlyStopping counter: 3 out of 7\n",
      "\n",
      "Starting epoch 26...\n",
      "[26/50]--------------------->train_loss: 0.89027   valid_loss: 0.24028\n",
      "EarlyStopping counter: 4 out of 7\n",
      "\n",
      "Starting epoch 27...\n",
      "[27/50]--------------------->train_loss: 0.89406   valid_loss: 0.24709\n",
      "EarlyStopping counter: 5 out of 7\n",
      "\n",
      "Starting epoch 28...\n",
      "[28/50]--------------------->train_loss: 0.87733   valid_loss: 0.25119\n",
      "EarlyStopping counter: 6 out of 7\n",
      "\n",
      "Starting epoch 29...\n",
      "[29/50]--------------------->train_loss: 0.87837   valid_loss: 0.24231\n",
      "EarlyStopping counter: 7 out of 7\n",
      "\n",
      "Early stopping----------STOPPED\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "epochs = 50\n",
    "hidden_dim1 = 64\n",
    "hidden_dim2 = 128\n",
    "patience = 7\n",
    "\n",
    "model = LSTMTagger(embedding_dim, hidden_dim1, hidden_dim2, len(word_to_ix), len(tag_to_ix), vectors)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model, train_loss, valid_loss = train_model(model, patience, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3wc9Z3/8ddHq1XvxcaybMv1gi03WbSEA1N/lFCSkICJc6GFC7m0H0d+kFx+gXDkdykECBdCAgmkERwCoRwhmMA5lEsosjHGBWPjKjfJsq1i1ZW+vz9mJK3llS1ZWq2kfT8fj33M7Ozs7Ge88rx3vjPzHXPOISIi8Ssh1gWIiEhsKQhEROKcgkBEJM4pCERE4pyCQEQkzikIRETinIJAxGdmJWbmzCyxD/NeZWavDXQ5IsOBgkBGJDPbYmatZlbQY/pKfyNcEpvKREYeBYGMZJuBRZ1PzGw2kBq7ckRGJgWBjGS/Af4p7PlngV+Hz2Bm2Wb2azOrNrOtZvZNM0vwXwuY2Z1mttfMNgEXRnjvL8xsl5ntMLM7zCzQ3yLNrMjMnjGzfWa20cw+F/baiWZWYWZ1ZrbHzO7yp6eY2W/NrMbMDpjZW2Y29mh1mdk0M3vZzGr99fp9f+uV+KMgkJHsdSDLzI73N4SXA7/tMc9/AtnAFOB0vOC42n/tc8BHgflAOXBZj/f+CggB0/x5zgWuO4Y6HwUqgSL/M/6fmZ3lv/Yj4EfOuSxgKvCYP/2zft0TgHzg80BTH+r6d+AFIBco9tdf5IgUBDLSde4VnAO8B+zofCEsHL7unKt3zm0Bfgh8xp/lU8A9zrntzrl9wH+EvXcscD7wVefcQedcFXA3cEV/ijOzCcCpwM3OuWbn3Erg52E1tAHTzKzAOdfgnHs9bHo+MM051+6cW+6cq+tDXW3AJKDI/7yIB7RFwikIZKT7DXAlcBU9moWAAiAJ2Bo2bSsw3h8vArb3eK3TJCAI7PKbZg4APwPG9LO+ImCfc66+lxquBWYA7/nNPx8NW6+lwBIz22lm3zezYB/q+j+AAW+a2Rozu6af9Uoc0ultMqI557aa2WbgAryNari9dP9CXutPm0j3XsMuvKYXwl7rtB1oAQqcc6EBlLgTyDOzzLAw6KrBObcBWOQft/g48LiZ5TvnDgLfBr7tnwH1HLDeH/Zal3NuN16TF2Z2KvCimb3inNs4gHWQUU57BDIaXAuc6W88uzjn2vHa3L9jZplmNgm4ke7jCI8BXzazYjPLBW4Je+8uvLb2H5pZlpklmNlUMzu9P4U557YDfwP+wz8APMev9xEAM1tsZoXOuQ7ggP+2djM7w8xm+81bdXiB1n60uszsk2ZW7C9nP+CA9v7ULPFHQSAjnnPuA+dcRS8vfwk4CGwCXgN+Bzzkv/YgXvPLO8AK4I893vtPeE1La/E2qo8D446hxEVACd7ewZPArc65v/ivnQesMbMGvAPHVzjnmoHj/M+rA9YBL9MdYEeq6wTgDX95zwBfcc5tPoaaJY6YbkwjIhLftEcgIhLnFAQiInFOQSAiEucUBCIicW7EXUdQUFDgSkpKYl2GiMiIsnz58r3OucJIr424ICgpKaGiorczBUVEJBIz29rba2oaEhGJcwoCEZE4pyAQEYlzI+4YgYgMjba2NiorK2lubo51KdIPKSkpFBcXEwwG+/weBYGIRFRZWUlmZiYlJSWYWazLkT5wzlFTU0NlZSWTJ0/u8/vUNCQiETU3N5Ofn68QGEHMjPz8/H7vxSkIRKRXCoGR51i+s7gJgvW76/nOn9bS1Kqu2UVEwsVNEFTub+TBVzezqvLA0WcWkZiqqalh3rx5zJs3j+OOO47x48d3PW9tbe3TMq6++mrWr1/f58/8+c9/zle/+tVjLXlEi5uDxWUTcwGo2Lqfk6bkx7gaETmS/Px8Vq5cCcBtt91GRkYGN9100yHzOOdwzpGQEPn37MMPPxz1OkeLuNkjyE1PYmphOsu37o91KSJyjDZu3EhpaSmf//znKSsrY9euXVx//fWUl5cza9Ysbr/99q55Tz31VFauXEkoFCInJ4dbbrmFuXPncsopp1BVVdXnz/ztb3/L7NmzKS0t5Rvf+AYAoVCIz3zmM13T7733XgDuvvtuZs6cydy5c1m8ePHgrnwUxc0eAUD5pDyWrt1NR4cjIUEHwUT66tv/tYa1O+sGdZkzi7K49aJZ/X7f2rVrefjhh/npT38KwHe/+13y8vIIhUKcccYZXHbZZcycOfOQ99TW1nL66afz3e9+lxtvvJGHHnqIW265JdLiD1FZWck3v/lNKioqyM7O5uyzz+bZZ5+lsLCQvXv38u677wJw4IDX5Pz973+frVu3kpSU1DVtJIjaHoGZPWRmVWa2+ijznWBm7WZ2WbRq6bRgUi4HGtvYtLch2h8lIlEydepUTjjhhK7njz76KGVlZZSVlbFu3TrWrl172HtSU1M5//zzAViwYAFbtmzp02e98cYbnHnmmRQUFBAMBrnyyit55ZVXmDZtGuvXr+crX/kKS5cuJTs7G4BZs2axePFiHnnkkX5d0BVr0dwj+CXwY+DXvc1gZgHge3g3EI+6BSXecYLlW/czbUzmUHykyKhwLL/coyU9Pb1rfMOGDfzoRz/izTffJCcnh8WLF0c8hz4pKalrPBAIEAqF+vRZvd3TPT8/n1WrVvHnP/+Ze++9lyeeeIIHHniApUuX8vLLL/P0009zxx13sHr1agKBQD/XcOhFbY/AOfcKsO8os30JeALoe4PdAEwpSCc3LUjFFh0nEBkN6urqyMzMJCsri127drF06eD+pjz55JNZtmwZNTU1hEIhlixZwumnn051dTXOOT75yU/y7W9/mxUrVtDe3k5lZSVnnnkmP/jBD6iurqaxsXFQ64mWmB0jMLPxwMeAM4ETjjLv9cD1ABMnThzIZ7JgUi7LtykIREaDsrIyZs6cSWlpKVOmTOEjH/nIgJb3i1/8gscff7zreUVFBbfffjsLFy7EOcdFF13EhRdeyIoVK7j22mtxzmFmfO973yMUCnHllVdSX19PR0cHN998M5mZI6PlwXrb9RmUhZuVAM8650ojvPYH4IfOudfN7Jf+fI/3nK+n8vJyN5Ab0/zkrxv5/vPrWfF/zyEvPenobxCJU+vWreP444+PdRlyDCJ9d2a23DlXHmn+WJ4+Wg4sMbMtwGXAT8zs0qh/6KQ8AFboNFIRESCGQeCcm+ycK3HOlQCPA19wzj0V7c+dU5xNMGBUKAhERIAoHiMws0eBhUCBmVUCtwJBAOfcT6P1uUeTEgwwqyhbewQiIr6oBYFzblE/5r0qWnVEUj4pl9+8vpXWUAdJiXFzcbWISERxuRVcMCmXllAHa3bWxroUEZGYi9sgANTvkIgIcRoEY7JSmJCXqiAQGaYWLlx42MVh99xzD1/4wheO+L6MjAwAdu7cyWWXRe61ZuHChRztFPR77rnnkIvBLrjggkHpO+i2227jzjvvHPByBltcBgF4p5FWbN3f6yXkIhI7ixYtYsmSJYdMW7JkCYsW9e3QY1FR0SEXhvVXzyB47rnnyMnJOeblDXdxGwRlk3Kprm+hcn9TrEsRkR4uu+wynn32WVpaWgDYsmULO3fu5NRTT6WhoYGzzjqLsrIyZs+ezdNPP33Y+7ds2UJpqXcda1NTE1dccQVz5szh8ssvp6mp+//8DTfc0NWF9a233grAvffey86dOznjjDM444wzACgpKWHv3r0A3HXXXZSWllJaWso999zT9XnHH388n/vc55g1axbnnnvuIZ9zNJGWefDgQS688ELmzp1LaWkpv//97wG45ZZbmDlzJnPmzDnsHg3HKq66oQ5XPqnzRjX7mJCXFuNqRIa5P98Cu98d3GUeNxvO/27El/Lz8znxxBN5/vnnueSSS1iyZAmXX345ZkZKSgpPPvkkWVlZ7N27l5NPPpmLL76413v13n///aSlpbFq1SpWrVpFWVlZ12vf+c53yMvLo729nbPOOotVq1bx5S9/mbvuuotly5ZRUFBwyLKWL1/Oww8/zBtvvIFzjpNOOonTTz+d3NxcNmzYwKOPPsqDDz7Ipz71KZ544ok+3ZOgt2Vu2rSJoqIi/vSnPwFeV9r79u3jySef5L333sPMBq2r67jdI5gxNpPM5ER1QCcyTIU3D4U3Cznn+MY3vsGcOXM4++yz2bFjB3v27Ol1Oa+88krXBnnOnDnMmTOn67XHHnuMsrIy5s+fz5o1ayJ2YR3utdde42Mf+xjp6elkZGTw8Y9/nFdffRWAyZMnM2/ePKB/XV33tszZs2fz4osvcvPNN/Pqq6+SnZ1NVlYWKSkpXHfddfzxj38kLW1wfsTG7R5BIMGYNzFHB4xF+qKXX+7RdOmll3LjjTeyYsUKmpqaun7JP/LII1RXV7N8+XKCwSAlJSURu54OF2lvYfPmzdx555289dZb5ObmctVVVx11OUc6ppicnNw1HggE+tw01NsyZ8yYwfLly3nuuef4+te/zrnnnsu3vvUt3nzzTV566SWWLFnCj3/8Y/77v/+7T59zJHG7RwDeaaTr99RT19wW61JEpIeMjAwWLlzINddcc8hB4traWsaMGUMwGGTZsmVs3br1iMs57bTTeOSRRwBYvXo1q1atArwurNPT08nOzmbPnj38+c9/7npPZmYm9fX1EZf11FNP0djYyMGDB3nyySf5x3/8xwGtZ2/L3LlzJ2lpaSxevJibbrqJFStW0NDQQG1tLRdccAH33HNP132dBypu9wjAO3PIOVi57QCnzSiMdTki0sOiRYv4+Mc/fsgZRJ/+9Ke56KKLKC8vZ968eXzoQx864jJuuOEGrr76aubMmcO8efM48cQTAZg7dy7z589n1qxZh3Vhff3113P++eczbtw4li1b1jW9rKyMq666qmsZ1113HfPnz+9zMxDAHXfc0XVAGLzbYUZa5tKlS/na175GQkICwWCQ+++/n/r6ei655BKam5txznH33Xf3+XOPJKrdUEfDQLuhDtfQEmLObUv54pnTufGcGYOyTJHRQt1Qj1wjqRvqmMtITuRDx2WpAzoRiWtxHQTgHSd4e9t+Qu0dsS5FRCQm4j4IyktyOdjazvo9hx8YEol3I63pWI7tO4v7ICibqA7oRCJJSUmhpqZGYTCCOOeoqakhJSWlX++L67OGAIpzUxmblczyrfv5p1NKYl2OyLBRXFxMZWUl1dXVsS5F+iElJYXi4uJ+vSfug8DMvA7odIWxyCGCwSCTJ0+OdRkyBOK+aQi8Duh2HGhid+2RryoUERmNFAR0d0Cn4wQiEo8UBMDMoixSggkKAhGJSwoCIBhIYG5xDsu37ot1KSIiQ05B4FswKZc1O+toam2PdSkiIkNKQeArL8kl1OF4p3JwbvQgIjJSRC0IzOwhM6sys9W9vP5pM1vlP/5mZnOjVUtf6MIyEYlX0dwj+CVw3hFe3wyc7pybA/w78EAUazmqnLQkpo3JUBCISNyJWhA4514Bej366pz7m3Ouc6v7OtC/S+GiYMHEXJZv3U9Hhy6pF5H4MVyOEVwL/Pmoc0XZgpJcapva2LS3IdaliIgMmZgHgZmdgRcENx9hnuvNrMLMKqLZ78kC/8IydTchIvEkpkFgZnOAnwOXOOdqepvPOfeAc67cOVdeWBi9W0pOKUgnNy2o4wQiEldiFgRmNhH4I/AZ59z7saojnJmxYFKugkBE4krUeh81s0eBhUCBmVUCtwJBAOfcT4FvAfnAT8wMINTb/TSH0oJJeby4rop9B1vJS0+KdTkiIlEXtSBwzi06yuvXAddF6/OP1YKwDujOmTk2xtWIiERfzA8WDzdzirMJBkzNQyISNxQEPaQEA5SOz1YHdCISNxQEESyYmMs7lbVU1elGNSIy+ikIIrjixIkEzPjSo28Tau+IdTkiIlGlIIhg2pgM7ri0lDc27+OeFzfEuhwRkahSEPTiEwuK+VR5Mff9dSMvvx+9q5lFRGJNQXAE3764lH8Ym8n//v1KdtU2xbocEZGoUBAcQWpSgPs+XUZLWztffvRt2nS8QERGIQXBUUwtzOD/fXw2b23Zz50vrI91OSIig05B0AeXzBvPlSdN5Gcvb+KldXtiXY6IyKBSEPTRtz46k5njsvjXP7zDjgM6XiAio4eCoI9SggF+8ukyQu2OL/5uBa0hHS8QkdFBQdAPJQXpfO8Tc3h72wG+9/x7sS5HRGRQKAj66cI54/jsKZP4xWubWbpmd6zLEREZMAXBMfjGhcczpzibm/7wDtv3Nca6HBGRAVEQHIPkxAD3XVkGwL/8bgUtofYYVyQicuwUBMdoQl4aP7hsLqsqa/m/T62muU1hICIjk4JgAM4rPY4bFk7lsYpKzr7rZZ57dxfOuViXJSLSLwqCAbr5vA/xu+tOIiM5kS88soLLH3id1TtqY12WiEifKQgGwYenFfDsl07lOx8rZWNVAxf9+DVufnwV1fUtsS5NROSoFASDJDGQwKdPmsSymxZy7Ucm88SKSs6486/c/9cPdDBZRIY1BcEgy04N8s2PzuSF/30aJ0/J43vPv8c5d73C86t36/iBiAxLCoIomVKYwc8/ewK/vuZEUoIJfP63y7nywTd4Y1ONbn8pIsOKjbRfqeXl5a6ioiLWZfRLqL2DR9/cxl1/eZ/9jW1kJify4Wn5nDq9kNOmFzApPz3WJYrIKGdmy51z5ZFeS4zihz4EfBSocs6VRnjdgB8BFwCNwFXOuRXRqieWEgMJfOaUEi6dP55XN+zl1Q3VvPL+Xpau8bq0npiXxqnTCzhtegGnTC0gOzUY44pFJJ5EbY/AzE4DGoBf9xIEFwBfwguCk4AfOedOOtpyR+IeQSTOObbUNHaFwuubamhoCZFgMHdCDv84vZBTpuQzf2IOKcFArMsVkRHuSHsEUW0aMrMS4NleguBnwF+dc4/6z9cDC51zu460zNESBD21tXewcvuBrj2Gd7YfoMNBMGDMHp/NCZPzOLEkjwWTcslJS4p1uSIywsSkaagPxgPbw55X+tMOCwIzux64HmDixIlDUtxQCwYSOKEkjxNK8rjxnBnUNrZRsXUfb23Zz1tb9vHQa5v52cubAPiHsZmcMDm3a/6inNQYVy8iI1ksg8AiTIu4e+KcewB4ALw9gmgWNVxkpwU56/ixnHX8WACa29pZuf0Ab23ex1tb9/PU2zv57evbABifk8qsoiymjcnoekwtzCA9OZZfr4iMFLHcUlQCE8KeFwM7Y1TLsJcSDHDylHxOnpIPeGcivbe7njc376Ni6z7W767nv9+rItTRnZNF2SlMDQuHaYXeMD8jOVarISLDUCyD4Bngi2a2BO9gce3Rjg9It8RAAqXjsykdn801p04GoDXUwbZ9B9lY1dD9qG5gyZvbaQrrHXVcdgpzirOZOyGHucU5zC7OJitFZyqJxKtonj76KLAQKDCzSuBWIAjgnPsp8BzeGUMb8U4fvTpatcSLpMQEpo3JZNqYzEOmd3Q4dtY2sbGqgQ17Gnh3Ry2rKg90nb4KMKUwnbnFOcwtzmbOhBxmjsvS2UoicUIXlMWxA42trKr0QmHl9lreqTzQ1VFeYoIxbUwGBRnJZKYkkpGcSGZKkIyURLJ6PM9MSSQ3LYmJeWkEEiId+hGRWBuuZw1JjOWkJXHajEJOm1EIeNc27K5r5p3tXjis21XHgaY29tQ109ASor45RENLqNflpSUFmFWUxayibGaPz2Z2cTZTCzMUDiLDnPYIpF86OhwNrSEamjuDoY365hBV9S2s3VnHuztqWbuzruuYRGowwMyiLGaPz2ZWURazi7OZVphBYmBwu7kKtXeQYEaCQkckIu0RyKBJSDCyUoJHPLjc3uHYVO0di3h3Ry2rd9TyWMV2Glu9cDCDjKTErmalQ5qZkjunec8DBvXNIepbQtQ3t1HnB1BdUxv1zV4I1TeHaGprJzs1yAkluZw4OY8TJ+czqyiL4CAHjshopD0CGRLtHY7NextYvaOOTXsP+nsUbV1NTp0b+ga/+akzNDolJyaQlRokM8ULjSw/RDKTg2SlesGxu66JNzbvY1P1QcBrqlowKZeT/GCYU5ytA+ASt7RHIDEXSLCIZzT1JtTewcGWdkIdHWSmBElK7Psv+6r6Zt7avJ83N9fwxuZ9/PAv7+Ocd1bVvAk5nDQ5j5njspiQl8bE/DSdOitxT3sEMuodaGylYst+3tyyjzc272P1jlrawy68y0kLMjEvzQuGHo9x2SmDfjxDJBa0RyBxLSctibNnjuXsmV53HQdbQmypOcj2fY1s63o0sXZnHS+s2U1be3dIBBKMwoxkCjP9R/h4j2nq0kNGKv3lStxJT05kVlE2s4qyD3utvcM7hXZbTWNXUFTVN1Nd30JVfTNrdtayt6H1kD2KTmlJAY7LTmFcdgrHZaV6w87n2SmMy04lNy2IdysOkeFDQSASJpBgjM9JZXxOKqdMzY84T3uHY39jK9X1Ld2Phhaq6lrYU9fMrtom/v7BXvbUtxwWGEmJCYzLTmFsVgr56UnkpCWRmxYkNy2J3HRvvHNaXnoSWSlBnRIrUacgEOmnQIJRkJFMQUYyx4/rfb72DsfehhZ21Tazu7bJHzZ7w7pmNlY1sL+xjf2NkfcwABKMrrOlMpKDZCQHyEhOJCMl6J92651+2/lIDibQ1u5oa+/wH/54yHve6j8PtXsH4WcVZVE6Ppvi3FTtqcQxBYFIlAQSjLFZ3q9/JuT0Op9zjvqWEPsPtnYFQ+f4gcZW9je2crClvesCvuqGFrbUNHY9b27r6HNNSYEEggEjMZBAQ0uoK4CyU4OUjs+itCi7qzPDSXlpfdob6ehw1DW3caCxjQNNbRxsCdHU2k5Tm/dobmunqbWdxlZ/3H/eEupgXHYK08dmMmOs1zNuWlJ0N0nOOVpCHdQ1tVHX3EZLqIPsVG8vLD0pMOAw7Fx+YoKNqJMMFAQiMWbWfZHepMitUUfUeaptfYu3YfM29gkkBoxgIIEkfzwxwQ7Z0DW3tbN+dz2rd9ayekcda3bW8vD/bKG13QuWjOTErqvCM1MSvQ19YysHmtoOGa9taqOvJx8mJyaQlhQgNRggmJjA86ubuz7PDIpzU5kxJrMrHGaMzWTamIxDrv9oCbVT1xSi1t+Y1za1eRv2Ju+Cw86NfF1TyB92XojoTev8vJ4SE4yctGBXMOSkeuPZaUFyUpNISkzoupK+oTnUtczw7lfqm9toa3ekJwUoL8nj5Cn5nDI1n9KirGEdDDp9VES6tIY62FBVz5oddX5A1LJ2Vx3NbR1kpiSS428Uc9K6N5a5aUGy/fGcNK/JKtXf2IcPUxIDh+1hhNo72LqvkQ176nl/TwPv76lnw54GNu1t6Dp7K8FgXHYqbe0d1DUffQ8oOTGBbL85LSvVC1hv6F+MmJrYNS0pkOCFSWMbB5pau/Zqwp/XNrZR7/exFUiwQ66G77wSPjOl80p5b/331DXz9w9q2FDVAHihekJJLqdM9e4pMqsoe8j74BrwPYvNbCpQ6ZxrMbOFwBy8m9IfGNRK+0BBIDK02jsczrkh/UXb1t7Blr0Hu8Jha81BUoIBsvxf6VmdG/mu5/4wNZHkxMG/erzzmEtqsH/NR9X1LbyxuYa/f1DD65tq+MC/6j0zOZETJ+dxytR8inJSCXU42js6CLU72juc/9x1T+9wtLc7yibl8pFpBce0DoMRBCuBcqAEWIp3U5l/cM5dcEwVDYCCQERGqqq6Zl7fvK8rGDbvPdiv9//z6VP4+vnHH9NnD8YFZR3OuZCZfQy4xzn3n2b29jFVIyISp8ZkpXDx3CIunlsEwJ66Zg40thFI8I7hBBKMxIA/TEg4dLo/jIa+BkGbmS0CPgtc5E9TBy0iIgPQdVZZjPW10e9q4BTgO865zWY2Gfht9MoSEZGh0qc9AufcWuDLAGaWC2Q6574bzcJERGRo9GmPwMz+amZZZpYHvAM8bGZ3Rbc0EREZCn1tGsp2ztUBHwceds4tAM6OXlkiIjJU+hoEiWY2DvgU8GwU6xERkSHW1yC4He/6gQ+cc2+Z2RRgQ/TKEhGRodLXg8V/AP4Q9nwT8IloFSUiIkOnrweLi83sSTOrMrM9ZvaEmRX34X3nmdl6M9toZrdEeH2imS0zs7fNbJWZDfmVyiIi8a6vTUMP43UrUQSMB/7Ln9YrMwsA9wHnAzOBRWY2s8ds3wQec87NB64AftL30kVEZDD0NQgKnXMPO+dC/uOXQOFR3nMisNE5t8k51wosAS7pMY8DsvzxbGBnH+sREZFB0tcg2Gtmi80s4D8WAzVHec94YHvY80p/WrjbgMVmVgk8B3wp0oLM7HozqzCziurq6j6WLCIifdHXILgG79TR3cAu4DK8bieOJFLvSD27Ol0E/NI5VwxcAPzGzA6ryTn3gHOu3DlXXlh4tB0RERHpjz4FgXNum3PuYudcoXNujHPuUryLy46kEpgQ9ryYw5t+rgUe8z/j70AKcGydbYuIyDEZyJ0mbjzK628B081sspkl4R0MfqbHPNuAswDM7Hi8IFDbj4jIEBpIEByxY2znXAj4It6FaOvwzg5aY2a3m9nF/mz/CnzOzN4BHgWuciPt3pkiIiPcQG5ef9QNtnPuObyDwOHTvhU2vhb4yABqEBGRATpiEJhZPZE3+AakRqUiEREZUkcMAudc5lAVIiIisTGQYwQiIjIKKAhEROKcgkBEJM4pCERE4pyCQEQkzikIRETinIJARCTOKQhEROKcgkBEJM4pCERE4pyCQEQkzikIRETinIJARCTOKQhEROKcgkBEJM4pCERE4pyCQEQkzsVPEGz4C9xbBgdrYl2JiMiwEj9BkJYH+z6AD16KdSUiIsNK/ATBuPmQXggbXoh1JSIiw0r8BEFCAkw7Bza+CB3tsa5GRGTYiJ8gAJh+DjTthx3LY12JiMiwEdUgMLPzzGy9mW00s1t6medTZrbWzNaY2e+iWQ9TzwALwPtLo/oxIiIjSdSCwMwCwH3A+cBMYJGZzewxz3Tg68BHnHOzgK9Gqx4AUnNhwkk6TiAiEiaaewQnAhudc5ucc63AEuCSHvN8DrjPObcfwDlXFbQ16FIAABC4SURBVMV6PNPPgd2roG5X1D9KRGQkiGYQjAe2hz2v9KeFmwHMMLP/MbPXzey8SAsys+vNrMLMKqqrqwdW1fRzveHGFwe2HBGRUSKaQWARprkezxOB6cBCYBHwczPLOexNzj3gnCt3zpUXFhYOrKqxsyCzSM1DIiK+aAZBJTAh7HkxsDPCPE8759qcc5uB9XjBED1mXvPQB8ugvS2qHyUiMhJEMwjeAqab2WQzSwKuAJ7pMc9TwBkAZlaA11S0KYo1eaafC631sO31qH+UiMhwF7UgcM6FgC8CS4F1wGPOuTVmdruZXezPthSoMbO1wDLga8656HcGNOV0SAiqeUhEBDDnejbbD2/l5eWuoqJi4Av61cXQUAX/or0CERn9zGy5c6480mvxdWVxuOnnQvU6OLAt1pWIiMRU/AbBjP/lDTf8JbZ1iIjEWPwGQf40yC1REIhI3IvfIDDzmoc2vwxtzbGuRkQkZuI3CMALgrZG2PparCsREYmZ+A6CklMhMUXNQyIS1+I7CIKpMPk0XU8gInEtvoMAvOahfZug5oNYVyIiEhMKgmlne0PtFYhInFIQ5E2GghkKAhGJWwoC8JqHtrwGrQdjXYmIyJBTEIDXLXV7K2x+JdaViIgMOQUBwMRTIClDzUMiEpcUBACJyTBloXc9wQjrjVVEZKAUBJ2mnwu126H6vVhXIiIypBQEnaaf4w3VPCQicUZB0CmrCMbOVncTIhJ3FAThpp8D2/4OzbWxrkREZMgoCMJNPxc6QvDBslhXIiIyZBQE4YpPgJRsNQ+JSFxREIQLJMLUs2DjX6CjI9bViIgMCQVBT9PPhYY9sHtVrCsRERkSCoKeunojVfOQiMQHBUFPGYVQVKbrCUQkbkQ1CMzsPDNbb2YbzeyWI8x3mZk5MyuPZj199g8XQOWb8PzXdWN7ERn1ohYEZhYA7gPOB2YCi8xsZoT5MoEvA29Eq5Z++/CX4ITPwes/gQfPgN2rY12RiEjURHOP4ERgo3Nuk3OuFVgCXBJhvn8Hvg8Mn5/ewRS48E648g9wcK8XBn/7sc4kEpFRKZpBMB7YHva80p/WxczmAxOcc88eaUFmdr2ZVZhZRXV19eBX2psZ58IX/g7TzoEX/g1+cwnU7hi6zxcRGQLRDAKLMK2rj2czSwDuBv71aAtyzj3gnCt3zpUXFhYOYol9kF4AVzwCF/8nVC6H+0+B1U8MbQ0iIlEUzSCoBCaEPS8GdoY9zwRKgb+a2RbgZOCZYXPAOJwZlP0TfP5VyJ8Oj18Df7xefRKJyKgQzSB4C5huZpPNLAm4Anim80XnXK1zrsA5V+KcKwFeBy52zlVEsaaByZ8K1yyFhV+Hdx+H+z8CW/4n1lWJiAxI1ILAORcCvggsBdYBjznn1pjZ7WZ2cbQ+N+oCibDwFrj2BQgE4ZcXwovfhvZQrCsTETkm5kbYrRnLy8tdRcUw2WloaYDnb4G3fwOTT4fLHob0/FhXJSJyGDNb7pyL2PSuK4sHIjkDLvkxXHIfbHsdHjgddr4d66pERPpFQTAY5i+Ga573bnz/i/8FK38X64pERPpMQTBYxpfBP78ME0+Cp26AP90EodZYVyUiclQKgsGUXgCLn/S6qHjrQfjVRVC/O9ZViYgckYJgsAUS4dw74LKHvHsa/Ox02DZ8ulESEelJQRAtpZ+A616EYKp3iumbD3rHEEREhhkFQTSNnQXXL4OpZ8BzN8HT/6JurUVk2EmMdQGjXmouLPo9/PU/4JXvw5qnIHs8ZBVBZpE3POQxHtLyvW4tRESGgIJgKCQkwJn/BpNOgfdfgLodUL8LNr/sHUx27YfOH0iCzHF+WBznjUcaJmfGZn1EZFRREAylqWd6j3Ad7dBQBXU7uwOibof3vH437H7XC4+2g4cvLymjOxgmnASzPuY1R2lvQkT6QUEQawkByBrnPVjQ+3wt9V4w1O86fHhgO7x2F7x6J+RPg5mXwqxLYWypQkFEjkpBMFIkZ3qPgumRX2+ohvf+yzsG0RkKeVO9QJj1MYWCiPRKnc6NRgf3wrr/gjVPwpZXwXV0h8LMS7zxpHQFg0gcOVKncwqC0a4zFNY+BZtf7T4wnRCElGxIzYGUnN6HaXnemU+puZDqjycmxXadRKTfjhQEahoa7dILoPxq73FwL2x8CRp2Q9N+aDoAzQe8YWMN1HzgPW+u9fYiepOU4QdDTnc4pOZCSpbfhJXtDbuehw1TsiAxeejWX0SOSkEQT9ILYO7lR5+vowNa672AaNrvP/Z1jzf2mFa11hs210F7y9GXnxCEpDQIpntXXneOJ6VB0H90jqcXQOGHvEduiXdwXUQGlYJADpeQ4DUbpWRD7qT+vTfU4p3h1FLnBUPneEu9/9wfb2vyToltbeweb66D+j3eeFuT91prffeyE1O8g+WdwTDmeAWEyCBQEMjgSkz2HukFg7O8lnqofh+q10HVOqhe790E6N0/hH2mHxD50yFvMuRNgdzJ3njGcV6w9Veoxbu+42A1pBd6V3wfy3Ii6Wj3muISU7xmtsFarnici+2JEM55P2Saa/0fRLVhjwPeD57waUnp3o+ZvMneMHey19Q6hOugIJDhLTkTihd4j3At9V4oVL/nB8R7sHMFrH360Cu1E1PD/pP54ZA3GRzQsMd/VMHBKm/YOa259tDPS0z1rtEomOYFTsF0//n0yFd4d7RD7XbYt8k79rJvsze+7wPYvwXaO+9VYd2nBkd8+Af0c0u8gMubMnQbCee8IDywHRr3Hrrx6tzjO2Rj5+/xZYyB4+bAuLne8LhSb2M32FobYe/7h/4NVL/n1Zua69WRMQbSx0QezxjrzZeQ2Pd/z7amQ6/hqdsV9nw31PsXgrY1Hnk5gSRvjzs5y/tbPlh16OvJ/t54VziUeH+/Y473LiIdZDprSEaX9rbuDfC+zd5Gd99m2L/ZG4aaDn9PUiZkFHobhq6NhT+elu/9J927EWo2wN4NcGDroQfTM8d5oZA7yTsgX+Nv7DvauudJTPU35JMhfypkFXvHU1rqIzSl1R/apNbacGi9Kdl+qE0Je/jPM8b2faMWavWuYq/dDrWV3ga0dlvYeGXvx3wCyd6B/86NWed4Uqa3vN2rvONGAJj37zNujh8Qc+C4uZHv7+2cF5KhZq+DxlCzt3fWWu99B9XroOo9b7h/K16i4x13yp8GYz4EOZO8YDok4Ksif/ddzGtetEDYMAEsoXtaqMX7Rd9TYorf7Yvf9UtWkfd3k5rj/9vkdDe1dj6CKYcuo/Wgtz77/b/Zrr/bLd7fW+cPhw9/yevm/hjo9FER8DYy9bu9/1wJAa/JJ2NM/3+thlq8oNm7wQ8HPyT2b/WWF76Bzp/qDTPHHfuv+Lam7o3Evk1hj81wYFuPvqrM/5ywIRw+zcxbLj3+/2eMhewJkDPBG3aOp4/xN2JZ3sat54asJ+e8kNm1yguFzmHt9u55Msd5zYihFq+WUIu34e9ZU7iERG+PbMyHDj1WlDcFAsEj19Pa0B0KDXu8vZ2m/d7em2vvMezoHnZOCyR1b+jD+/xKyYnuHlpHh7ensX+L9z0UzjimxSgIREar9jYvDDr3eBr2+Pe9cGH3v3A9pvnTg2mHbvSzxh99Az9Qjfu6g6FqrbehTUz2flV3PZK9s8nCpwfTuoP1SBt86ZWuIxAZrQJBb+OYPzXWlfRNWh5MWeg9ZNiI6ukKZnaema03s41mdkuE1280s7VmtsrMXjKzfp6rKCIiAxW1IDCzAHAfcD4wE1hkZjN7zPY2UO6cmwM8Dnw/WvWIiEhk0dwjOBHY6Jzb5JxrBZYAl4TP4Jxb5pzrPM/qdaA4ivWIiEgE0QyC8UDYKQJU+tN6cy3w50gvmNn1ZlZhZhXV1dWDWKKIiEQzCCKdTxXxFCUzWwyUAz+I9Lpz7gHnXLlzrrywsHAQSxQRkWieNVQJTAh7Xgzs7DmTmZ0N/BtwunOuDz2WiYjIYIrmHsFbwHQzm2xmScAVwDPhM5jZfOBnwMXOuaoIyxARkSiLWhA450LAF4GlwDrgMefcGjO73cwu9mf7AZAB/MHMVprZM70sTkREomTEXVlsZtXA1mN8ewGwdxDLGU5G67ppvUae0bpuI329JjnnIh5kHXFBMBBmVtHbJdYj3WhdN63XyDNa1220rhdE+cpiEREZ/hQEIiJxLt6C4IFYFxBFo3XdtF4jz2hdt9G6XvF1jEBERA4Xb3sEIiLSg4JARCTOxU0QHO3eCCOVmW0xs3f9C/JG9K3bzOwhM6sys9Vh0/LM7C9mtsEf5sayxmPRy3rdZmY7/O9tpZldEMsaj4WZTTCzZWa2zszWmNlX/Omj4Tvrbd1G/PcWSVwcI/DvjfA+cA5eH0hvAYucc2tjWtggMLMtePd0GMkXugBgZqcBDcCvnXOl/rTvA/ucc9/1AzzXOXdzLOvsr17W6zagwTl3ZyxrGwgzGweMc86tMLNMYDlwKXAVI/87623dPsUI/94iiZc9gqPeG0Fizzn3CrCvx+RLgF/547/C+884ovSyXiOec26Xc26FP16P15XMeEbHd9bbuo1K8RIE/b03wkjigBfMbLmZXR/rYqJgrHNuF3j/OYExMa5nMH3Rv03rQyOx+SScmZUA84E3GGXfWY91g1H0vXWKlyDo870RRqCPOOfK8G4J+i9+M4QMf/cDU4F5wC7gh7Et59iZWQbwBPBV51xdrOsZTBHWbdR8b+HiJQj6dG+Ekcg5t9MfVgFP4jWDjSZ7/PbaznbbUdFduXNuj3Ou3TnXATzICP3ezCyIt6F8xDn3R3/yqPjOIq3baPneeoqXIDjqvRFGIjNL9w9kYWbpwLnA6iO/a8R5BvisP/5Z4OkY1jJoOjeUvo8xAr83MzPgF8A659xdYS+N+O+st3UbDd9bJHFx1hCAf5rXPUAAeMg5950YlzRgZjYFby8AvLvN/W4kr5eZPQosxOvudw9wK/AU8BgwEdgGfNI5N6IOvPayXgvxmhccsAX458529ZHCzE4FXgXeBTr8yd/Aa0sf6d9Zb+u2iBH+vUUSN0EgIiKRxUvTkIiI9EJBICIS5xQEIiJxTkEgIhLnFAQiInFOQSDiM7P2sF4lVw5mL7VmVhLe+6jIcJIY6wJEhpEm59y8WBchMtS0RyByFP49H75nZm/6j2n+9Elm9pLfAdlLZjbRnz7WzJ40s3f8x4f9RQXM7EG/f/sXzCzVn//LZrbWX86SGK2mxDEFgUi31B5NQ5eHvVbnnDsR+DHeFer44792zs0BHgHu9affC7zsnJsLlAFr/OnTgfucc7OAA8An/Om3APP95Xw+Wisn0htdWSziM7MG51xGhOlbgDOdc5v8jsh2O+fyzWwv3s1L2vzpu5xzBWZWDRQ751rCllEC/MU5N91/fjMQdM7dYWbP49245ingKedcQ5RXVeQQ2iMQ6RvXy3hv80TSEjbeTvcxuguB+4AFwHIz07E7GVIKApG+uTxs+Hd//G94PdkCfBp4zR9/CbgBvNukmllWbws1swRggnNuGfB/gBzgsL0SkWjSLw+RbqlmtjLs+fPOuc5TSJPN7A28H0+L/GlfBh4ys68B1cDV/vSvAA+Y2bV4v/xvwLuJSSQB4Ldmlo13A6W7nXMHBm2NRPpAxwhEjsI/RlDunNsb61pEokFNQyIicU57BCIicU57BCIicU5BICIS5xQEIiJxTkEgIhLnFAQiInHu/wNrpVnTQ/VO+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.title('Model losses')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train Loss', 'Validation Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCrothnófar|VERB|VERB\n",
      "\tPól|NOUN|PROPN\n",
      "\tnó|CCONJ|SCONJ\n",
      "\tba|AUX|AUX\n",
      "\túrlabhraí|NOUN|NOUN\n",
      "\tmaith|ADJ|ADJ\n",
      "\té|PRON|PRON\n",
      "\tar|ADP|ADP\n",
      "\tRaidió|NOUN|PROPN\n",
      "\tna|DET|DET\n",
      "\tGaeltachta|PROPN|PROPN\n",
      "\tagus|CCONJ|CCONJ\n",
      "\tna|DET|DET\n",
      "\tmeáin|NOUN|NOUN\n",
      "\teile|DET|DET\n",
      "\tag|ADP|ADP\n",
      "\tcosaint|PROPN|NOUN\n",
      "\tna|DET|DET\n",
      "\tn-oifigí|PROPN|NOUN\n",
      "\tpoist|PROPN|NOUN\n",
      "\ttuaithe|PRON|NOUN\n",
      "\t,|PUNCT|PUNCT\n",
      "\tgné|PROPN|NOUN\n",
      "\tam-tábhachtach|PROPN|ADJ\n",
      "\tde|ADP|ADP\n",
      "\tshaol|NOUN|NOUN\n",
      "\tsóisialta|NOUN|ADJ\n",
      "\tna|DET|DET\n",
      "\tndaoine|NOUN|NOUN\n",
      "\t.|PUNCT|PUNCT\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = torch.LongTensor(prepare_sequence(test_data[0][0], word_to_ix))\n",
    "    tag_scores = model(inputs.view(-1,inputs.shape[0]))\n",
    "\n",
    "    #print(tag_scores)\n",
    "    # Print the actual words with their tags\n",
    "    for i,word in enumerate(test_data[0][0]):\n",
    "        j = int(np.argmax(tag_scores[i]))\n",
    "        print(f\"\\t{word}|{ix_to_tag[j]}|{test_data[0][1][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6991150442477876"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = []\n",
    "true = []\n",
    "for i in range(len(test_data[0][1])):\n",
    "    pred.append(ix_to_tag[int(np.argmax(tag_scores[i]))])\n",
    "    true.append(test_data[0][1][i])\n",
    "\n",
    "compute_accuracy_test(pred,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # this will be the file to write the outputs\n",
    "    with open(\"data/mymodel_output_irish.output\", 'w', encoding = \"utf-8\") as op:\n",
    "        for instance in test_data:\n",
    "            # Convert the test sentence into a word ID tensor\n",
    "            inputs = torch.LongTensor(prepare_sequence(instance[0], word_to_ix))\n",
    "            # Forward pass\n",
    "            tag_scores = model(inputs.view(-1,inputs.shape[0]))\n",
    "            # Find the tag with the highest probability in each position\n",
    "            outputs = [int(np.argmax(ts)) for ts in tag_scores]\n",
    "            # Prepare the output to be written in the same format as the test file (word|tag)\n",
    "            formatted_output = ' '.join([f\"{word}|{ix_to_tag[tag_id]}\" for word,tag_id in zip(instance[0],outputs)])\n",
    "            # Write the output\n",
    "            op.write(formatted_output + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Reading the file====================\n",
      "====================Reading the file====================\n",
      "0.8083885646453656\n"
     ]
    }
   ],
   "source": [
    "#Give the path of the OUTPUT file\n",
    "OUTPUT_FILE = \"data/mymodel_output_irish.output\"\n",
    "output = read_data(OUTPUT_FILE)\n",
    "\n",
    "#Give the path of the ORIGINAL file\n",
    "REFERENCE_FILE = 'data/irish.test'\n",
    "gold = read_data(REFERENCE_FILE)\n",
    "\n",
    "acc = compute_accuracy(output,gold)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
